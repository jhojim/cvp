{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae0be6eefa3374",
   "metadata": {},
   "source": [
    "### COURSE WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c913b-f591-4b86-9b95-a9cd00c5e2f0",
   "metadata": {},
   "source": [
    "The purpose of this work is to create a tool that will speed up the processing of military video reports before publication on the network, namely, the anonymization of faces, voice changes, and watermarking.\n",
    "In general, we have accumulated a bunch of typical videos that are waiting for processing and anonymization, the videos are usually very similar to each other and represent the appearance of one person or a group showing something and saying a small text. Sometimes there are dynamic shots, but mostly it is a more static video. On the downside, almost all videos have very poor image quality.\n",
    "\n",
    "Libraries used\n",
    "cv2 and dlib are for processing and handling images/videos. The face_recognition and mtcnn libraries will be used for detecting faces in the video frames. librosa and soundfile are used for audio manipulation, and moviepy.editor is required for handling the video file and its audio track. These libraries provide the functions and methods necessary to fulfill the objectives of the code.\n",
    "\n",
    "All work can be divided into two parts - processing of the video itself (pixelation of faces and watermarking) and sound processing.\n",
    "\n",
    "Video processing, initially we use a detector to find faces in the frame, for this we have a choice between two detectors (face_recognition or MTCNN), the first one we can use for videos with good quality, it works quite fast, but it does not have the same recognition clarity as the MTCNN that we use for videos with a poor image or faces that are partially covered or in shadow. But using MTCNN, we will get an increase in processing time along with the accuracy of the detector.\n",
    "\n",
    "In order to be sure that all faces are found, we can specify the expected number of faces in the video, and the detector will trigger in each frame until we find the expected number.\n",
    "We can also forcefully specify that the dector should trigger every N frames or if the picture in the frame has undergone major changes compared to the previous one (for example, a sudden change of frame or the appearance of something new in the frame) - for this we use Mean Squared Error (MSE) is a popular method in the field of image processing and computer vision for quantifying the difference between two images. In this specific context, it is used to determine how much content has changed between two frames of a video.\n",
    "\n",
    "After we have detected the necessary faces, we initialize the trackers. We pixelate each region of detected face in an image and add a watermark to the frame if necessary.\n",
    "After processing the entire video, we save it in the temp folder.\n",
    "\n",
    "Since OpenCV does not support audio and we need not only to get the sound, but also to change the voices on it to anonymize them, we use moviepy editor in order to take the sound from the original video and with the librosa pitch_shift effect, we shift the pitch of the audio and after add this sound to the new processed video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53ff5455a17021ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import dlib\n",
    "import face_recognition\n",
    "from tqdm import tqdm\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "\n",
    "# Parameters for figure size\n",
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1f389ac-d635-4674-bb1b-b2cd3d8fb7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "VIDEO_SOURCE = 'data/video_1.mp4' # the path to the original video file to process\n",
    "WATERMARK_SOURCE = 'data/watermark.png'  # the path to watermark png file (set None for no watermark)\n",
    "DETECTOR = 'mtcnn_detector'  # Detector type (face_recognition, mtcnn_detector)\n",
    "THRESHOLD = 0  # Threshold value for the Mean Squared Error (set to 0 for disabling this feature)\n",
    "MAX_FRAMES = 0  # Number of frames to wait before re-detecting faces (set to 0 for disabling this feature) \n",
    "MIN_FACES = 2  # Minimum number of detecting faces (set to 0 for disabling this feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f17b420-ba7a-44ac-9ffb-ba1f7b6365cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MTCNN for face detection\n",
    "if DETECTOR == 'mtcnn_detector':\n",
    "    mtcnn_detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "789c91ec-ae74-4317-a6a4-67e1d1c0e2fd",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Open video file\n",
    "video = cv2.VideoCapture(VIDEO_SOURCE)\n",
    "\n",
    "# Obtain properties of the video\n",
    "frame_rate = video.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Create a VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('temp/output.avi', fourcc, frame_rate, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99e5459-4590-466b-b9b4-e7de04e4e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the WATERMARK_SOURCE is not None to proceed with adding a watermark to the video\n",
    "if WATERMARK_SOURCE:\n",
    "    # Load the watermark image with the alpha channel (-1 indicates loading the image with alpha channel)\n",
    "    watermark = cv2.imread(WATERMARK_SOURCE, -1)\n",
    "    # Resize the watermark image to a desirable size; here we are resizing it to 100x100 pixels\n",
    "    watermark = cv2.resize(watermark, (100, 100))\n",
    "    \n",
    "    # Get the dimensions of the watermark image\n",
    "    (wH, wW) = watermark.shape[:2]\n",
    "    # Split the watermark image into four channels: Blue, Green, Red, and Alpha (transparency)\n",
    "    # using the cv2.split() function\n",
    "    (wB, wG, wR, wA) = cv2.split(watermark)\n",
    "    # Merge back the Blue, Green, and Red channels excluding the Alpha channel\n",
    "    watermark_rgb = cv2.merge([wB, wG, wR])\n",
    "    # Create a binary mask of the alpha channel by thresholding it\n",
    "    _, alpha_mask = cv2.threshold(wA, 0, 255, cv2.THRESH_BINARY)\n",
    "    # Convert this grayscale alpha_mask to a three-channel image \n",
    "    watermark_mask = cv2.cvtColor(alpha_mask, cv2.COLOR_GRAY2BGR)\n",
    "    # Obtain the inverse of the watermark_mask\n",
    "    # This will be used to black-out the area where the watermark will be placed in the input images\n",
    "    watermark_mask_inv = cv2.bitwise_not(watermark_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ae9c512-b0f0-477b-8eb5-60d4967d6aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Mean Squared Error between two images\n",
    "def mse(imageA, imageB):\n",
    "    # Subtract imageB from imageA, square the difference,\n",
    "    # and then summation of the squares is calculated \n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "\n",
    "    # The summation calculated earlier is then divided by the total number of\n",
    "    # pixels in the image to get the Mean Squared Error (MSE)\n",
    "    # This is equivalent to averaging all squared differences\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1] * imageA.shape[2])\n",
    "\n",
    "    # Return the MSE\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "067a9ec9-ccf4-4d87-87d4-e655785759b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pixelate region in an image\n",
    "def pixelate_region(image, top_left, bottom_right, pix_size):\n",
    "    # Get the coordinates\n",
    "    x1, y1 = top_left\n",
    "    x2, y2 = bottom_right\n",
    "\n",
    "    # Break the region into small square slices (each of size pix_size x pix_size)\n",
    "    for y in range(y1, y2, pix_size):\n",
    "        for x in range(x1, x2, pix_size):\n",
    "            # For each slice calculate the average colour\n",
    "            # The size check is to ignore the pixels where size < `pix_size`x`pix_size` at the borders (if any)\n",
    "            if image[y:y + pix_size, x:x + pix_size].size > 0:\n",
    "                # Calculate the average colour of a `pix_size`x`pix_size` block\n",
    "                avg_color = np.mean(np.mean(image[y:y + pix_size, x:x + pix_size], axis=0), axis=0)\n",
    "            else:\n",
    "                continue # If size is smaller than a block, it is ignored\n",
    "\n",
    "            # Replace the original pixels at this block with the average colour, effectively pixelating it\n",
    "            image[y:y + pix_size, x:x + pix_size] = avg_color\n",
    "\n",
    "    # Return the pixelated image\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af03fa1-804e-43ea-aeec-c76954e5c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect faces and initialize trackers\n",
    "def detect_faces_and_initialize_trackers(detector, frame, previous_frame=None, threshold=0):\n",
    "    # Use MTCNN detector if selected\n",
    "    if detector == 'mtcnn_detector':\n",
    "        # Perform face detection using MTCNN\n",
    "        face_locations = mtcnn_detector.detect_faces(frame)\n",
    "        # Initialization of Correlation trackers for each detected face\n",
    "        trackers = [dlib.correlation_tracker() for _ in range(len(face_locations))]\n",
    "        # For each detected face, setup the tracker\n",
    "        for face, tracker in zip(face_locations, trackers):\n",
    "            # Extract bounding box parameters from the current face\n",
    "            (x, y, w, h) = face['box']\n",
    "            # Define a Dlib rectangle object from the bounding box parameters\n",
    "            rect = dlib.rectangle(x, y, x + w, y + h)\n",
    "            # Start the tracker on the current face\n",
    "            tracker.start_track(frame, rect)\n",
    "\n",
    "    # Use Face Recognition detector if it is selected \n",
    "    elif detector == 'face_recognition':\n",
    "        # Use a CNN model for face detection in the initial frame \n",
    "        # or if the Mean Squared Error with the previous frame exceeds the threshold\n",
    "        if initial_frame or (previous_frame is not None and threshold > 0 and mse(frame, previous_frame) > threshold):\n",
    "            face_locations = face_recognition.face_locations(frame, model=\"cnn\")  # use CNN model here\n",
    "        else:\n",
    "            # Use the default face_recognition face detector\n",
    "            face_locations = face_recognition.face_locations(frame)\n",
    "\n",
    "        # Initialize correlation trackers\n",
    "        trackers = [dlib.correlation_tracker() for _ in range(len(face_locations))]\n",
    "        # For each detected face, setup the tracker\n",
    "        for face, tracker in zip(face_locations, trackers):\n",
    "            # Convert the bounding box parameters to Dlib rectangle\n",
    "            top, right, bottom, left = face\n",
    "            rect = dlib.rectangle(left, top, right, bottom)\n",
    "            # Start the tracker on the face\n",
    "            tracker.start_track(frame, rect)\n",
    "\n",
    "    # Return the face locations and initialized trackers\n",
    "    return face_locations, trackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33787457-3779-40f6-80e6-a54d8f5a9353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/268 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [00:06<00:00, 41.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "initial_frame = True # Boolean flag to indicate if the current frame is the initial frame\n",
    "previous_frame = None # Stores the previous frame of the video\n",
    "iteration = 0 # Counter to keep track of the current frame number\n",
    "trackers = [] # List to store tracker objects for each detected face\n",
    "\n",
    "# A lambda function to determine whether face detection should be performed on the current frame.\n",
    "# The face detection will be executed if any of the following conditions is true:\n",
    "# 1. It's the initial frame.\n",
    "# 2. Current frame number is a multiple of MAX_FRAMES.\n",
    "# 3. The number of face trackers is less than MIN_FACES.\n",
    "# 4. Mean Squared Error between the current and previous frame exceeds the THRESHOLD.\n",
    "tracker_condition = lambda: (initial_frame or\n",
    "                             MAX_FRAMES > 0 and iteration % MAX_FRAMES == 0 or\n",
    "                             MIN_FACES > 0 and len(trackers) < MIN_FACES or\n",
    "                             (previous_frame is not None and THRESHOLD > 0 and mse(frame, previous_frame) > THRESHOLD))\n",
    "\n",
    "# Iterate over frames and process each one\n",
    "for _ in tqdm(range(total_frames)):\n",
    "    # Read a frame from the video.\n",
    "    ok, frame = video.read()\n",
    "    \n",
    "    # If video frame is not read successfully, then we reach the end, break the loop.\n",
    "    if not ok:\n",
    "        break\n",
    "\n",
    "    # If the condition defined in tracker_condition is True, detect faces and update trackers\n",
    "    if tracker_condition():\n",
    "        face_locations, trackers = detect_faces_and_initialize_trackers(DETECTOR, frame, previous_frame, THRESHOLD)\n",
    "        initial_frame = False\n",
    "\n",
    "    # Store current frame as the previous frame for the next iteration\n",
    "    previous_frame = frame.copy()\n",
    "    \n",
    "    # Update each tracker and pixelate each detected face in the current frame\n",
    "    for tracker in trackers:\n",
    "        tracker.update(frame)\n",
    "        \n",
    "        # Obtain the position of the tracked object\n",
    "        pos = tracker.get_position()\n",
    "        \n",
    "        # Convert the position data to integer format\n",
    "        x = int(pos.left())\n",
    "        y = int(pos.top())\n",
    "        w = int(pos.width())\n",
    "        h = int(pos.height())\n",
    "\n",
    "        # Pixelate the face region in the frame\n",
    "        frame = pixelate_region(frame, (x, y), (x + w, y + h), 8)\n",
    "\n",
    "    # If WATERMARK_SOURCE is not None, apply the watermark to the frame\n",
    "    if WATERMARK_SOURCE:\n",
    "        # Define the region of interest for the watermark image in the original image\n",
    "        roi = frame[0:wH, 0:wW]\n",
    "    \n",
    "        # Black-out the area behind the logo in our original ROI\n",
    "        img1_bg = cv2.bitwise_and(roi, watermark_mask_inv)\n",
    "    \n",
    "        # Mask out the watermark from its image\n",
    "        img2_fg = cv2.bitwise_and(watermark_rgb, watermark_mask)\n",
    "    \n",
    "        # Merge these two to create the final watermark\n",
    "        dst = cv2.add(img1_bg, img2_fg)\n",
    "    \n",
    "        # And place the result back into the original image roi\n",
    "        frame[0:wH, 0:wW] = dst\n",
    "\n",
    "    # Increment the frame counter\n",
    "    iteration += 1\n",
    "    \n",
    "    # Write the frame to the output video file.\n",
    "    out.write(frame)\n",
    "\n",
    "# When everything is done, release the captures\n",
    "out.release()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c59119-e0a7-4419-abba-ff6c12331e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in temp/temp_audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video result/output.mp4.\n",
      "MoviePy - Writing audio in outputTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video result/output.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready result/output.mp4\n"
     ]
    }
   ],
   "source": [
    "# Manipulate audio in the original video to produce a pitch-shifted version\n",
    "\n",
    "# Extract and modify audio from original video\n",
    "# Load the source video file using moviepy's VideoFileClip\n",
    "clip = VideoFileClip(VIDEO_SOURCE)\n",
    "# Save the audio from the source video to a temporary MP3 file using moviepy's write_audiofile\n",
    "clip.audio.write_audiofile(\"temp/temp_audio.mp3\")\n",
    "# Use librosa to load the temporary audio file, extracting the audio time series and sampling rate\n",
    "y, sr = librosa.load('temp/temp_audio.mp3')\n",
    "# Shift the pitch of the audio using librosa. -4 semitones is equivalent to decreasing the pitch by two tones\n",
    "y_shifted = librosa.effects.pitch_shift(y, sr=sr, n_steps=-4, bins_per_octave=12)  # shift pitch\n",
    "# Write the modified audio to a WAV file using the soundfile library. WAV was chosen as it is lossless \n",
    "# and widely accepted by most audio and video applications\n",
    "sf.write('temp/temp_shifted_audio.wav', y_shifted, sr)\n",
    "\n",
    "# Combine the modified audio with the original video\n",
    "# Load the modified audio file\n",
    "shifted_audio = AudioFileClip('temp/temp_shifted_audio.wav')\n",
    "# Load the original video file without audio\n",
    "videoclip = VideoFileClip('temp/output.avi')\n",
    "# Replace the original video's audio with the modified audio\n",
    "videoclip.audio = shifted_audio\n",
    "# Write the final result to an output video file\n",
    "videoclip.write_videofile(\"result/output.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f37c0-d087-402f-97f4-af453a629f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
